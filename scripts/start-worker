#!/bin/bash

echo "The umask is set to $(umask)"

if [ -z "$SPARK_MASTER_ADDR" ] \
  || [ -z "$SPARK_PORT_DRIVER" ] \
  || [ -z "$SPARK_PORT_FILESERVER" ] \
  || [ -z "$SPARK_PORT_BROADCAST" ] \
  || [ -z "$SPARK_PORT_REPL_CLASS_SERVER" ] \
  || [ -z "$SPARK_PORT_BLOCK_MANAGER" ] \
  || [ -z "$SPARK_PORT_EXECUTOR" ] \
  || [ -z "$SPARK_WORKER_PORT" ] \
  || [ -z "$SPARK_WORKER_WEBUI_PORT" ] \
  || [ -z "$SPARK_LOCAL_IP" ] \
  || [ -z "$SPARK_USER_NAME" ] \
  || [ -z "$SPARK_USER_ID" ] \
  || [ -z "$SPARK_GROUP_NAME" ] \
  || [ -z "$SPARK_GROUP_ID" ] \
  || [ -z "$SPARK_WORKER_MEMORY" ] \
  || [ -z "$SPARK_WORKER_CORES" ] \
  || [ -z "$SPARK_SHUFFLE_SERVICE_ENABLED" ] \
  || [ -z "$SPARK_SHUFFLE_SERVICE_PORT" ]
then
  echo "Please set all required environment variables."
  exit 1
fi

echo "running setup-users"

setup-users

echo "adding configuration to $SPARK_HOME/conf/spark-env.sh"

echo "User id without sudo: $(id)"

echo "User id with sudo: $(sudo id)"

echo "" | sudo tee -a "$SPARK_HOME"/conf/spark-env.sh
echo "umask $SPARK_WORKER_UMASK" | sudo tee -a "$SPARK_HOME"/conf/spark-env.sh
echo "SPARK_EXECUTOR_JAVA_OPTS=-XX:+UseG1GC -XX:MaxGCPauseMillis=1000" | sudo tee -a "$SPARK_HOME"/conf/spark-env.sh

echo "exporting SPARK_WORKER_OPTS"

export SPARK_WORKER_OPTS="$SPARK_WORKER_OPTS -Dspark.driver.port=$SPARK_PORT_DRIVER \
-Dspark.fileserver.port=$SPARK_PORT_FILESERVER \
-Dspark.broadcast.port=$SPARK_PORT_BROADCAST \
-Dspark.replClassServer.port=$SPARK_PORT_REPL_CLASS_SERVER \
-Dspark.blockManager.port=$SPARK_PORT_BLOCK_MANAGER \
-Dspark.executor.port=$SPARK_PORT_EXECUTOR \
-Dspark.broadcast.factory=org.apache.spark.broadcast.HttpBroadcastFactory \
-Dspark.shuffle.service.enabled=$SPARK_SHUFFLE_SERVICE_ENABLED \
-Dspark.shuffle.service.port=$SPARK_SHUFFLE_SERVICE_PORT \
-XX:+UseG1GC \
-XX:MaxGCPauseMillis=1000"

echo "$SPARK_WORKER_OPTS"

echo "checking event log dir $SPARK_EVENT_LOG_DIR"

if [ "$SPARK_EVENT_LOG_DIR" ] && [ -d "$SPARK_EVENT_LOG_DIR" ]; then

    sudo chown -R "$SPARK_USER_NAME":"$SPARK_GROUP_NAME" "$SPARK_EVENT_LOG_DIR"
    # make sure local dir gets a 'file://' prefix
    if [[ "$SPARK_EVENT_LOG_DIR" == /* ]]; then SPARK_EVENT_LOG_DIR="file://$SPARK_EVENT_LOG_DIR"; fi
    export SPARK_WORKER_OPTS="$SPARK_WORKER_OPTS -Dspark.eventLog.enabled=true -Dspark.eventLog.dir=$SPARK_EVENT_LOG_DIR"

else

    echo "skip history config"

fi

echo "checking worker cleanup parameter $SPARK_WORKER_CLEANUP_ENABLED"

if [ "$SPARK_WORKER_CLEANUP_ENABLED" == "true" ]; then
    export SPARK_WORKER_OPTS="$SPARK_WORKER_OPTS -Dspark.worker.cleanup.enabled=true"
    if [ "$SPARK_WORKER_CLEANUP_INTERVAL" ]
    then
        export SPARK_WORKER_OPTS="$SPARK_WORKER_OPTS -Dspark.worker.cleanup.interval=$SPARK_WORKER_CLEANUP_INTERVAL"
    fi
    if [ "$SPARK_WORKER_CLEANUP_APPDATATTL" ]
    then
        export SPARK_WORKER_OPTS="$SPARK_WORKER_OPTS -Dspark.worker.cleanup.appDataTtl=$SPARK_WORKER_CLEANUP_APPDATATTL"
    fi
fi

echo "using worker opts: $SPARK_WORKER_OPTS"

sudo chown -R "$SPARK_USER_NAME":"$SPARK_GROUP_NAME" "$SPARK_HOME"

echo "Trying to sudo to Spark user:"
echo "sudo -u $SPARK_USER_NAME id"
echo "Sudo worked"

sudo -u "$SPARK_USER_NAME" id

echo "exec sudo -E -u $SPARK_USER_NAME -g $SPARK_GROUP_NAME $SPARK_HOME/bin/spark-class org.apache.spark.deploy.worker.Worker \
    spark://$SPARK_MASTER_ADDR \
    -h $SPARK_LOCAL_IP \
    -p $SPARK_WORKER_PORT \
    --memory $SPARK_WORKER_MEMORY \
    --cores $SPARK_WORKER_CORES \
    --webui-port $SPARK_WORKER_WEBUI_PORT \
    $@"

exec sudo -E -u "$SPARK_USER_NAME" -g "$SPARK_GROUP_NAME" "$SPARK_HOME"/bin/spark-class org.apache.spark.deploy.worker.Worker \
    spark://"$SPARK_MASTER_ADDR" \
    -h "$SPARK_LOCAL_IP" \
    -p "$SPARK_WORKER_PORT" \
    --memory "$SPARK_WORKER_MEMORY" \
    --cores "$SPARK_WORKER_CORES" \
    --webui-port "$SPARK_WORKER_WEBUI_PORT" \
    "$@"
